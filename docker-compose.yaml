services:
  uiuc-chat-frontend:
    build:
      context: ./uiuc-chat-frontend
      args:
        NEXT_PUBLIC_KEYCLOAK_URL: ${NEXT_PUBLIC_KEYCLOAK_URL}
        NEXT_PUBLIC_KEYCLOAK_REALM: ${NEXT_PUBLIC_KEYCLOAK_REALM}
        NEXT_PUBLIC_KEYCLOAK_CLIENT_ID: ${NEXT_PUBLIC_KEYCLOAK_CLIENT_ID}
    container_name: uiuc-chat-frontend
    networks:
      - uiuc-chat-network
      - supabase_default
    ports:
      - ${FRONTEND_PORT}:3000
    expose:
      - ${FRONTEND_PORT}
    env_file:
      - .env

  redis:
    image: redis:7.2
    restart: unless-stopped
    container_name: redis
    command: redis-server --requirepass ${INGEST_REDIS_PASSWORD}
    # ports:
    #   - 6379:6379
    networks:
      - uiuc-chat-network
    volumes:
      - redis-data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "ping" ]
      interval: 30s
      timeout: 10s
      retries: 3

  qdrant:
    image: qdrant/qdrant:v1.12.6
    restart: unless-stopped
    container_name: qdrant
    # environment:
    #   - QDRANT_API_KEY=${QDRANT_API_KEY}
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    volumes:
      - ./qdrant_data:/qdrant/storage
      - ./qdrant_config.yaml:/qdrant/config/production.yaml # Mount the config file directly as a volume
    networks:
      - uiuc-chat-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "-H", "Authorization: Bearer ${QDRANT_API_KEY}", "http://qdrant:6333/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  minio:
    image: minio/minio:RELEASE.2024-06-13T22-53-53Z
    restart: unless-stopped
    container_name: minio
    # Customize env vars in .env file
    environment:
      MINIO_ROOT_USER: ${AWS_ACCESS_KEY_ID}
      MINIO_ROOT_PASSWORD: ${AWS_SECRET_ACCESS_KEY}
      MINIO_API_PORT: ${DOCKER_INTERNAL_MINIO_API_PORT}
      MINIO_CONSOLE_PORT: ${DOCKER_INTERNAL_MINIO_DASHBOARD_PORT}
    command: server /data --console-address ":${DOCKER_INTERNAL_MINIO_DASHBOARD_PORT}" --address ":${DOCKER_INTERNAL_MINIO_API_PORT}"
    ports:
      - ${PUBLIC_MINIO_API_PORT}:${DOCKER_INTERNAL_MINIO_API_PORT} # API access
      - ${PUBLIC_MINIO_DASHBOARD_PORT}:${DOCKER_INTERNAL_MINIO_DASHBOARD_PORT} # Dashboard access
    expose:
      - ${PUBLIC_MINIO_API_PORT}
      - ${PUBLIC_MINIO_DASHBOARD_PORT}
    networks:
      - uiuc-chat-network
    volumes:
      - minio-data:/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://minio:${DOCKER_INTERNAL_MINIO_API_PORT}/minio/health/live" ]
      interval: 30s
      timeout: 10s
      retries: 3

  flask-app:
    build: . # Directory with Dockerfile for Flask app
    # image: kastanday/ai-ta-backend:gunicorn
    restart: unless-stopped
    container_name: flask-app
    ports:
      - "${FLASK_PORT}:8001"
    expose:
      - ${FLASK_PORT}
    volumes:
      - ./db:/usr/src/app/db # Mount local directory to store SQLite database
    networks:
      - uiuc-chat-network
      - supabase_default # Add connection to Supabase network
    depends_on:
      - qdrant
      - redis
      - minio
    env_file:
      - .env
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://flask-app:8001" ]
      interval: 30s
      timeout: 10s
      retries: 3

  ingest-worker:
    build: . # Use the same build context as the Flask app
    command: python ai_ta_backend/redis_queue/worker.py
    restart: unless-stopped
    container_name: ingest-worker
    networks:
      - uiuc-chat-network
      - supabase_default
    depends_on:
      - redis
    env_file:
      - .env
    healthcheck:
      test: [ "CMD", "python", "-c", "from redis import Redis; from rq import Worker; r = Redis(host='redis', port=6379, password='${INGEST_REDIS_PASSWORD}'); exit(0 if Worker.count(r) > 0 else 1)" ]
      interval: 30s
      timeout: 10s
      retries: 3

  crawlee:
    build: ./ic_crawlee
    container_name: ic_crawlee
    networks:
      - uiuc-chat-network
    volumes:
      - ./data:/data
    ports:
      - "3345:3000"
    env_file:
      - .env
    environment:
      - INGEST_URL=http://flask-app:8001/ingest
    depends_on:
      - redis

  keycloak:
    platform: linux/amd64  # M chip
    image: quay.io/keycloak/keycloak:26.1
    container_name: keycloak
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "8080:8080"
    networks:
      - uiuc-chat-network
      - supabase_default
    command:
      - start-dev
      - --http-enabled=true
      - --hostname=localhost
      - --hostname-strict=false
      - --db=postgres
      - --db-url=jdbc:postgresql://supabase-db:5432/postgres
      - --db-username=${POSTGRES_USER}
      - --db-password=${POSTGRES_PASSWORD}
      - --import-realm
    environment:
      - KEYCLOAK_ADMIN=${KEYCLOAK_ADMIN_USERNAME}
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD}
      - KC_METRICS_ENABLED=true
      - KC_HEALTH_ENABLED=true
    volumes:
      - ./keycloak/keywind:/opt/keycloak/themes/keywind
      - ./keycloak/realms/realm-export.json:/opt/keycloak/data/import/realm-export.json
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://keycloak:8080/health/ready" ]
      interval: 30s
      timeout: 10s
      retries: 3

# declare the network resource
# this will allow you to use service discovery and address a container by its name from within the network
networks:
  uiuc-chat-network:
    driver: bridge
  supabase_default:
    external: true # Mark as external since it's managed by Supabase

volumes:
  redis-data: {}
  qdrant-data: {}
  minio-data: {}
